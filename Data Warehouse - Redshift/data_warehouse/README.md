# Project: Data Warehouse

## Introduction 

In this project, I applied learning in data warehouses and AWS to build an ETL pipeline that extracts data from S3 to test tables hosted on Redshift and creates the analysis tables from these test tables.

## Datasets

### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. Below is an example:

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

And below is an example of what the data:

![Drag Racing](https://video.udacity-data.com/topher/2019/February/5c6c3ce5_log-data/log-data.png)

## Schema

### Fact Table
#### songplay

    songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

### Dimension Tables 
#### user

    user_id, first_name, last_name, gender, level

##### song

    song_id, title, artist_id, year, duration

##### artist

    artist_id, name, location, lattitude, longitude

##### time

    start_time, hour, day, week, month, year, weekday
    
## Steps to execute the Python scripts

- Fill the <b> dwh.cfg <b> with HOST, DB_NAME, DB_USER, DB_PASSWORD, DB_PORT and ARN;
- Run <b> create_tables.py <b> to drop and create tables;
- Run <b> etl.py <b> to create the ETL pipeline.
