# Project: Data Lake

In this project, the learnings in Apache Spark and data lakes were applied to build an ETL pipeline for a data lake hosted on S3. To complete the project, data was loaded from S3, processed in analytical tables using Spark and loaded back into S3. This Spark process was deployed in a cluster using AWS.

# Datasets

## Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. Here is an example of a song file looks like.

``` {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0} ```

## Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings. Here is an example of a song file looks like.

![Log_Data](https://video.udacity-data.com/topher/2019/February/5c6c3f0a_log-data/log-data.png)

# ETL Pipeline

### Extract data from S3
```Song data: s3://udacity-dend/song_data```
```Log data: s3://udacity-dend/log_data```

### Transform data using spark
##### Fact Table
- songplays_table(songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)

##### Dimension Tables
- users_table(user_id, first_name, last_name, gender, level)

- songs_table(song_id, title, artist_id, year, duration)

- artists_table(artist_id, name, location, lattitude, longitude)

- time_table(start_time, hour, day, week, month, year, weekday)

### Load back to S3

Write data in partitioned parquet files in S3.